{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENPM673 Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0 Frames per second\n"
     ]
    }
   ],
   "source": [
    "#importing modules\n",
    "from ast import Pass\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Creating a VideoCapture object and reading from input file\n",
    "cap = cv2.VideoCapture('proj2_v2.mp4')\n",
    "#capturing the fps of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "#creating frame counter\n",
    "frame_counter=0\n",
    "print(f\"{fps} Frames per second\")\n",
    "\n",
    "# Checking if camera opened successfully\n",
    "if (cap.isOpened()== False):\n",
    "  print(\"Error opening video stream or file\")\n",
    "\n",
    "# Reading until video is completed\n",
    "while(cap.isOpened()):\n",
    "  # Captureing frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    "    #incrementing frame counter\n",
    "    \n",
    "    #Converting BGR to Graysclae to reduce the channel of colors\n",
    "    image=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Variance of Laplacian\n",
    "    # laplcian=cv2.Laplacian(frame,cv2.CV_64F)\n",
    "    # laplacian_var=cv2.Laplacian(image,cv2.CV_64F).var()\n",
    "\n",
    "    #******************************\n",
    "    # Apply identity kernel\n",
    "    kernel1 = np.array([[0, 1, 0],\n",
    "                        [1, -4, 1],\n",
    "                        [0, 1, 0]])\n",
    " \n",
    "    identity = cv2.filter2D(src=image, ddepth=-1, kernel=kernel1)\n",
    "    identity_var=identity.var()\n",
    "    #updating variable text based on the threshold of variance of laplacian\n",
    "    if identity_var>150:\n",
    "        text=\"not blurry\"\n",
    "        frame_counter+=1\n",
    "        #putting text on the frame\n",
    "        # cv2.putText(frame, \"{}: {:.2f}\".format(text, laplacian_var), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 3)\n",
    "        #Smoothing the frames to eliminate the noise from the video frames\n",
    "        # gblur = cv2.GaussianBlur(image, (15, 15), 0)\n",
    "        # #--------------------------------------------------------------------\n",
    "        # #Binary thresolding a grayscale frame to capture the moving dark object\n",
    "        # ret,thres=cv2.threshold(image,205,255,cv2.THRESH_BINARY)\n",
    "        # thres_lower = 500 # Lower Threshold \n",
    "        # thres_upper = 1520 # Upper threshold \n",
    "        \n",
    "        # # Applying the Canny Edge filter with L2Gradient = True \n",
    "        # edge = cv2.Canny(thres, thres_lower, thres_upper) \n",
    "        # lines = cv2.HoughLinesP(\n",
    "        #     edge, # Input edge image\n",
    "        #     1, # Distance resolution in pixels\n",
    "        #     np.pi/180, # Angle resolution in radians\n",
    "        #     threshold=70, # Min number of votes for valid line\n",
    "        #     minLineLength=80, # Min allowed length of line\n",
    "        #     maxLineGap=10 # Max allowed gap between line for joining them\n",
    "        #     )\n",
    "        # # Iterate over points\n",
    "        # for points in lines:\n",
    "        #       # Extracted points nested in the list\n",
    "        #     x1,y1,x2,y2=points[0]\n",
    "        #     # Draw the lines joing the points\n",
    "        #     # On the original image\n",
    "        #     cv2.line(frame,(x1,y1),(x2,y2),(0,255,0),4)\n",
    "        #-------------------------------------------------------------------------------------------\n",
    "        # cv2.imshow('Frame',identity)\n",
    "    elif identity_var<150:\n",
    "      text=\"Blurry\"\n",
    "    #putting text on the frame\n",
    "    cv2.putText(identity, \"{}: {:.2f}\".format(text, identity_var), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 3)\n",
    "    cv2.imshow('Frame',identity)\n",
    "       \n",
    "    # print(laplacian_var)\n",
    "    # Press Q on keyboard to  exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "      break\n",
    "\n",
    "  # Break the loop\n",
    "  else:\n",
    "    break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Apply identity kernel\n",
    "kernel1 = np.array([[0, 0, 0],\n",
    "                    [0, 1, 0],\n",
    "                    [0, 0, 0]])\n",
    " \n",
    "identity = cv2.filter2D(src=image, ddepth=-1, kernel=kernel1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
    "# We convert the resolutions from float to integer.\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    " \n",
    "# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\n",
    "out = cv2.VideoWriter('outpy.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    " \n",
    " #in while loop\n",
    " # Write the frame into the file 'output.avi'\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0 Frames per second\n"
     ]
    }
   ],
   "source": [
    "#importing modules\n",
    "from ast import Pass\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def findIntersection(points1,points2):\n",
    "        x1,y1,x2,y2=points1[0]\n",
    "        x3,y3,x4,y4=points2[0]\n",
    "        deno_x=(x1-x2)*(y3-y4)-(y1-y2)*(x3-x4)\n",
    "        num_x=(x1*y2-y1*x2)*(x3-x4)-(x1-x2)*(x3*y4-y3*x4) \n",
    "        deno_y=(x1-x2)*(y3-y4)-(y1-y2)*(x3-x4)\n",
    "        num_y=(x1*y2-y1*x2)*(y3-y4)-(y1-y2)*(x3*y4-y3*x4)\n",
    "        if deno_x!=0:\n",
    "          px= ( num_x ) / ( deno_x ) \n",
    "        else:\n",
    "           return ['nan','nan' ]\n",
    "        if deno_y!=0:\n",
    "          py= ( num_y ) / ( deno_y )\n",
    "        else:\n",
    "           return ['nan','nan' ]\n",
    "        if np.isinf(px) or np.isinf(py):\n",
    "           return ['nan','nan' ]\n",
    "        else:\n",
    "          return [int(px), int(py)]\n",
    "\n",
    "# Creating a VideoCapture object and reading from input file\n",
    "cap = cv2.VideoCapture('proj2_v2.mp4')\n",
    "#capturing the fps of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "#creating frame counter\n",
    "frame_counter=0\n",
    "print(f\"{fps} Frames per second\")\n",
    "\n",
    "# Checking if camera opened successfully\n",
    "if (cap.isOpened()== False):\n",
    "  print(\"Error opening video stream or file\")\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    " \n",
    "# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\n",
    "out = cv2.VideoWriter('outvideo.mp4',cv2.VideoWriter_fourcc(*'mp4v'), 10, (frame_width,frame_height))\n",
    "\n",
    "# Reading until video is completed\n",
    "while(cap.isOpened()):\n",
    "  # Captureing frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    "    #incrementing frame counter\n",
    "    \n",
    "    #Converting BGR to Graysclae to reduce the channel of colors\n",
    "    image=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Variance of Laplacian\n",
    "    # laplcian=cv2.Laplacian(frame,cv2.CV_64F)\n",
    "    # laplacian_var=cv2.Laplacian(image,cv2.CV_64F).var()\n",
    "\n",
    "    #******************************\n",
    "    # Apply identity kernel\n",
    "    kernel= np.array([[1, 1, 1],\n",
    "                        [1, 1, 1],\n",
    "                        [1, 1, 1]])\n",
    "    kernel1 = np.array([[0, 1, 0],\n",
    "                        [1, -4, 1],\n",
    "                        [0, 1, 0]])\n",
    "    kernel2 = np.array([[0, -1, 0],\n",
    "                        [-1, 4, -1],\n",
    "                        [0, -1, 0]])\n",
    " \n",
    "    identity = cv2.filter2D(src=image, ddepth=cv2.CV_64F, kernel=kernel1)\n",
    "    identity_var=identity.var()\n",
    "    #updating variable text based on the threshold of variance of laplacian\n",
    "    if identity_var>150:\n",
    "        text=\"not blurry\"\n",
    "        frame_counter+=1\n",
    "        #putting text on the frame\n",
    "        # cv2.putText(frame, \"{}: {:.2f}\".format(text, laplacian_var), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 3)\n",
    "        #Smoothing the frames to eliminate the noise from the video frames\n",
    "        # gblur = cv2.GaussianBlur(image, (15, 15), 0)\n",
    "        # #--------------------------------------------------------------------\n",
    "        # #Binary thresolding a grayscale frame to capture the moving dark object\n",
    "        thres=np.where(image>=205, 255,0).astype(np.uint8)\n",
    "        # ret,thres=cv2.threshold(image,205,255,cv2.THRESH_BINARY)\n",
    "        img_ero = cv2.erode(thres, kernel, iterations=1)\n",
    "        img_dil = cv2.dilate(img_ero, kernel, iterations=1)\n",
    "\n",
    "        thres_lower = 80 # Lower Threshold \n",
    "        thres_upper = 240 # Upper threshold \n",
    "        \n",
    "        # Applying the Canny Edge filter with L2Gradient = True \n",
    "        edge = cv2.Canny(img_dil, thres_lower, thres_upper) \n",
    "\n",
    "        # This returns an array of r and theta values\n",
    "        # lines = cv2.HoughLines(edge, 1, np.pi/180, 70)\n",
    "        \n",
    "        # # print(lines)\n",
    "        # # The below for loop runs till r and theta values\n",
    "        # # are in the range of the 2d array\n",
    "        # for r_theta in lines:\n",
    "        #     arr = np.array(r_theta[0], dtype=np.float64)\n",
    "        #     r, theta = arr\n",
    "        #     # Stores the value of cos(theta) in a\n",
    "        #     a = np.cos(theta)\n",
    "        \n",
    "        #     # Stores the value of sin(theta) in b\n",
    "        #     b = np.sin(theta)\n",
    "        \n",
    "        #     # x0 stores the value rcos(theta)\n",
    "        #     x0 = a*r\n",
    "        \n",
    "        #     # y0 stores the value rsin(theta)\n",
    "        #     y0 = b*r\n",
    "        \n",
    "        #     # x1 stores the rounded off value of (rcos(theta)-1000sin(theta))\n",
    "        #     x1 = int(x0 + 1000*(-b))\n",
    "        \n",
    "        #     # y1 stores the rounded off value of (rsin(theta)+1000cos(theta))\n",
    "        #     y1 = int(y0 + 1000*(a))\n",
    "        \n",
    "        #     # x2 stores the rounded off value of (rcos(theta)+1000sin(theta))\n",
    "        #     x2 = int(x0 - 1000*(-b))\n",
    "        \n",
    "        #     # y2 stores the rounded off value of (rsin(theta)-1000cos(theta))\n",
    "        #     y2 = int(y0 - 1000*(a))\n",
    "\n",
    "        #     print(x1,y1,x2,y2)\n",
    "        #     # cv2.line draws a line in img from the point(x1,y1) to (x2,y2).\n",
    "        #     # (0,0,255) denotes the colour of the line to be\n",
    "        #     # drawn. In this case, it is red.\n",
    "        #     cv2.line(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        lines = cv2.HoughLinesP(\n",
    "            edge, # Input edge image\n",
    "            1, # Distance resolution in pixels\n",
    "            np.pi/180, # Angle resolution in radians\n",
    "            threshold=60, # Min number of votes for valid line\n",
    "            minLineLength=100, # Min allowed length of line\n",
    "            maxLineGap=5# Max allowed gap between line for joining them\n",
    "            )\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # Iterate over points\n",
    "        for points in lines:\n",
    "              # Extracted points nested in the list\n",
    "            x1,y1,x2,y2=points[0]\n",
    "            # Draw the lines joing the points\n",
    "            # On the original image\n",
    "            x=0\n",
    "            y=((y2-y1)/(x2-x1))*(-x2+x)+y2\n",
    "            x_1=x\n",
    "            y_1=int(y)\n",
    "\n",
    "            x=720\n",
    "            y=((y2-y1)/(x2-x1))*(-x2+x)+y2\n",
    "           \n",
    "            x_2=x\n",
    "            y_2=int(y)\n",
    "         \n",
    "\n",
    "            # _lines.append([x_1,y_1,x_2,y_2])\n",
    "\n",
    "            cv2.line(frame,(int (x_1),int (y_1)),(int (x_2),int (y_2)),(255,0,0),1)\n",
    "            # cv2.line(frame,(int (x1),int (y1)),(int (x2),int (y2)),(0,255,0),4)\n",
    "        # print(len(lines))\n",
    "        for i in range(len(lines)-1):\n",
    "           for j in range(len(lines)-i-1):\n",
    "              corner=findIntersection(lines[i],lines[j+i+1])\n",
    "              if corner[0]=='nan' or corner[1]=='nan':\n",
    "                  continue\n",
    "              else:\n",
    "                frame = cv2.circle(frame, tuple(corner), 5, (0,0,255), -1)\n",
    "        \n",
    "        \n",
    "        img_dil = np.float32(img_dil) \n",
    "\n",
    "        # apply the cv2.cornerHarris method \n",
    "        # to detect the corners with appropriate \n",
    "        # values as input parameters \n",
    "        dest = cv2.cornerHarris(img_dil, 5, 3, 0.07)\n",
    "        \n",
    "        frame[dest > 0.01 * dest.max()]=[0, 255, 0] \n",
    "\n",
    "        \n",
    "        # frame[dest > 0.01 * dest.max()]=[0, 255, 0] \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        #-------------------------------------------------------------------------------------------\n",
    "        # print(_lines)\n",
    "        # cv2.imshow('Frame',frame)\n",
    "    \n",
    "    #putting text on the frame\n",
    "    # cv2.putText(identity, \"{}: {:.2f}\".format(text, identity_var), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 3)\n",
    "    # cv2.imshow('Frame',identity)\n",
    "    # cv2.imshow('Frame',frame) \n",
    "\n",
    "    out.write(frame)\n",
    "    # print(laplacian_var)\n",
    "    # Press Q on keyboard to  exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "      break\n",
    "\n",
    "  # Break the loop\n",
    "  else:\n",
    "    break\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harris COrner Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0 Frames per second\n"
     ]
    }
   ],
   "source": [
    "# Python program to illustrate \n",
    "# corner detection with \n",
    "# Harris Corner Detection Method \n",
    "\n",
    "# organizing imports \n",
    "import cv2 \n",
    "import numpy as np \n",
    "\n",
    "# Creating a VideoCapture object and reading from input file\n",
    "cap = cv2.VideoCapture('proj2_v2.mp4')\n",
    "#capturing the fps of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "#creating frame counter\n",
    "frame_counter=0\n",
    "print(f\"{fps} Frames per second\")\n",
    "\n",
    "# Checking if camera opened successfully\n",
    "if (cap.isOpened()== False):\n",
    "  print(\"Error opening video stream or file\")\n",
    "\n",
    "# Reading until video is completed\n",
    "while(cap.isOpened()):\n",
    "  # Captureing frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    "    #incrementing frame counter\n",
    "    \n",
    "    #Converting BGR to Graysclae to reduce the channel of colors\n",
    "    image=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Variance of Laplacian\n",
    "    # laplcian=cv2.Laplacian(frame,cv2.CV_64F)\n",
    "    # laplacian_var=cv2.Laplacian(image,cv2.CV_64F).var()\n",
    "\n",
    "    #******************************\n",
    "    # Apply identity kernel\n",
    "    kernel1 = np.array([[0, 1, 0],\n",
    "                        [1, -4, 1],\n",
    "                        [0, 1, 0]])\n",
    " \n",
    "    identity = cv2.filter2D(src=image, ddepth=-1, kernel=kernel1)\n",
    "    identity_var=identity.var()\n",
    "    #updating variable text based on the threshold of variance of laplacian\n",
    "    if identity_var>90:\n",
    "        text=\"not blurry\"\n",
    "        frame_counter+=1\n",
    "         # #Binary thresolding a grayscale frame to capture the moving dark object\n",
    "        ret,thres=cv2.threshold(image,205,255,cv2.THRESH_BINARY)\n",
    "        img_ero = cv2.erode(thres, kernel, iterations=1)\n",
    "        img_dil = cv2.dilate(img_ero, kernel, iterations=1)\n",
    "        # modify the data type \n",
    "        # setting to 32-bit floating point \n",
    "        img_dil = np.float32(img_dil) \n",
    "\n",
    "        # apply the cv2.cornerHarris method \n",
    "        # to detect the corners with appropriate \n",
    "        # values as input parameters \n",
    "        dest = cv2.cornerHarris(img_dil, 2, 5, 0.05) \n",
    "\n",
    "        # Results are marked through the dilated corners \n",
    "        # dest = cv2.dilate(dest, None) \n",
    "\n",
    "        # Reverting back to the original image, \n",
    "        # with optimal threshold value \n",
    "        frame[dest > 0.01 * dest.max()]=[0, 0, 255] \n",
    "\n",
    "        # the window showing output image with corners \n",
    "        cv2.imshow('Image with Borders', frame) \n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "       break\n",
    "\n",
    "  # Break the loop\n",
    "  else:\n",
    "    break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canny edge with the trackbar\n",
    "\n",
    "##### Upper threshold: All the pixel values above threshold limit is considered\n",
    "##### Lower Threshold: All the Pixel value below Threshold limit is discarded\n",
    "##### Values in between Upp-Lower Threshold : considered only if the connected pixel is from upper thresold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "def something(x):\n",
    "    pass\n",
    "# Create a black image, a window\n",
    "img = cv.imread('book.jpg',0)\n",
    "img=cv.resize(img,(600,400),cv.INTER_AREA)\n",
    "cv.namedWindow('image')\n",
    "# create trackbars for color change\n",
    "cv.createTrackbar('min','image',0,1000,something)\n",
    "cv.createTrackbar('max','image',0,1000,something)\n",
    "# create switch for ON/OFF functionality\n",
    "switch = \"0 : OFF \\n 1 : ON\"\n",
    "cv.createTrackbar(switch, 'image',0,1,something)\n",
    "while(1):\n",
    "    # get current positions of four trackbars\n",
    "    g = cv.getTrackbarPos ('min', 'image')\n",
    "    b = cv.getTrackbarPos ('max', 'image')\n",
    "    s = cv.getTrackbarPos (switch, 'image')\n",
    "    if s == 0:\n",
    "        img=img\n",
    "        edges=img\n",
    "    if s==1:\n",
    "        edges = cv.Canny(img,g,b)\n",
    "    cv.imshow ('image', edges)\n",
    "    k = cv.waitKey (1) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "2 3\n",
      "2 4\n",
      "2 5\n",
      "3 4\n",
      "3 5\n",
      "4 5\n"
     ]
    }
   ],
   "source": [
    "pin=[1,2,3,4,5]\n",
    "for i in range(len(pin)-1):\n",
    "    for j in range(len(pin)-i-1):\n",
    "        print(pin[i],pin[j+1+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "### Homography and stitching an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualising SIFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load an image\n",
    "image = cv2.imread('PA120274.JPG')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "\n",
    "# Draw keypoints on the image\n",
    "image_with_keypoints = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('SIFT Keypoints', image_with_keypoints)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matching SIFT features between 2 consecutive images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two images\n",
    "image1 = cv2.imread('PA120275.JPG')\n",
    "image2 = cv2.imread('PA120274.JPG')\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Draw matches\n",
    "matched_image = cv2.drawMatches(image1, keypoints1, image2, keypoints2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Matched Features', matched_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing Homography between the pair of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizeImage(image, percentage):\n",
    "  \"\"\"Function to resize an image by given percentage\n",
    "  \"\"\"\n",
    "  original_height, original_width = image.shape[:2]\n",
    "\n",
    "  new_height = int(original_height * (percentage / 100))\n",
    "  new_width = int(original_width * (percentage / 100))\n",
    "\n",
    "  resized_img = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "  return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.41215807e-01  5.59591722e-02  1.64579418e+02]\n",
      " [-1.15982217e-01  9.73796679e-01  1.21197103e+01]\n",
      " [-5.50690447e-04  6.61437139e-05  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two images\n",
    "image_1 = cv2.imread('PA120272.JPG')\n",
    "image_2 = cv2.imread('PA120273.JPG')\n",
    "\n",
    "# Resize the image\n",
    "image1 = resizeImage(image_1, 50)\n",
    "# Resize the image\n",
    "image2 = resizeImage(image_2, 50)\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract keypoints for the good matches\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 2)\n",
    "\n",
    "# Estimate homography using RANSAC\n",
    "homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "print(homography)\n",
    "\n",
    "# Apply homography to image1 to warp it onto image2\n",
    "height, width, _ = image1.shape\n",
    "warped_image1 = cv2.warpPerspective(image1, homography, (image2.shape[1] +image1.shape[1], height))\n",
    "warped_image1[0:image2.shape[0],0:image2.shape[1]]=image2\n",
    "\n",
    "\n",
    "def trim(frame):\n",
    "    if not np.sum(frame[0]):\n",
    "        return trim(frame[1:])\n",
    "    if not np.sum(frame[-1]):\n",
    "        return trim(frame[:-2])\n",
    "    if not np.sum(frame[:,0]):\n",
    "        return trim(frame[:,1:])\n",
    "    if not np.sum(frame[:,-1]):\n",
    "        return trim(frame[:,:-2])\n",
    "    return frame\n",
    "\n",
    "# Blending the warped image with the second image using alpha blending\n",
    "# alpha = 0.5  # blending factor\n",
    "# blended_image = cv2.addWeighted(warped_image1, alpha, image2, 1 - alpha, 0)\n",
    "\n",
    "# Create a mask to blend the images smoothly\n",
    "# mask = np.zeros((height, width), dtype=np.uint8)\n",
    "# mask2 = np.zeros((height, width+width), dtype=np.uint8)\n",
    "# cv2.fillConvexPoly(mask, np.int32(dst_pts.squeeze()), (255, 255, 255), cv2.LINE_AA)\n",
    "\n",
    "# # Inverse mask for image1\n",
    "# mask_inv = cv2.bitwise_not(mask)\n",
    "\n",
    "# # Apply the mask to both images\n",
    "# image2_masked = cv2.bitwise_and(image2, image2, mask=mask_inv)\n",
    "# warped_image1_masked = cv2.bitwise_and(warped_image1, warped_image1, mask=mask2)\n",
    "\n",
    "# # Blend the images together\n",
    "# result = cv2.add(image2_masked, warped_image1_masked)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply homography to the keypoints of image1\n",
    "# transformed_pts = cv2.perspectiveTransform(src_pts, homography)\n",
    "\n",
    "# # Count inliers\n",
    "# num_inliers = np.sum(mask)\n",
    "\n",
    "# # Draw matches\n",
    "# matched_image = cv2.drawMatches(image1, keypoints1, image2, keypoints2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Draw inliers on the matched image\n",
    "# for i in range(len(mask)):\n",
    "#     if mask[i] == 1:\n",
    "#         pt1 = tuple(map(int, src_pts[i].reshape(2)))\n",
    "#         pt2 = tuple(map(int, transformed_pts[i].reshape(2)))\n",
    "#         cv2.line(matched_image, pt1, pt2, (0, 255, 0), 1)\n",
    "\n",
    "warped_image1__=trim(warped_image1)\n",
    "# Display the result\n",
    "cv2.imshow('RANSAC Matches', warped_image1__)\n",
    "cv2.imwrite('warped_image23.jpg', warped_image1__)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2) (112, 2)\n",
      "112 112\n",
      "[[ 13.521971 197.02095 ]\n",
      " [ 15.014202 231.55392 ]\n",
      " [ 18.472614 293.27002 ]\n",
      " [ 27.992794 270.18985 ]\n",
      " [ 30.612354 234.868   ]\n",
      " [ 31.443373 208.47368 ]\n",
      " [ 33.27267  285.2789  ]\n",
      " [ 33.43179  430.09103 ]\n",
      " [ 34.233555 204.38515 ]\n",
      " [ 41.317837 272.5342  ]\n",
      " [ 41.929157 304.5829  ]\n",
      " [ 44.14183  312.3727  ]\n",
      " [ 45.20043  226.2013  ]\n",
      " [ 47.605602 294.7123  ]\n",
      " [ 51.35366  224.32056 ]\n",
      " [ 51.786575 204.03345 ]\n",
      " [ 51.90937  195.85257 ]\n",
      " [ 54.08631  316.38266 ]\n",
      " [ 54.18366  320.46777 ]\n",
      " [ 55.20024  349.0737  ]\n",
      " [ 55.411427 349.31927 ]\n",
      " [ 55.439045 210.30658 ]\n",
      " [ 55.790733 201.94955 ]\n",
      " [ 55.848488 246.93167 ]\n",
      " [ 59.05109  309.04358 ]\n",
      " [ 64.69841  266.6563  ]\n",
      " [ 65.67086  197.1364  ]\n",
      " [ 66.4149   207.56323 ]\n",
      " [ 67.75143  247.84235 ]\n",
      " [ 74.024895 201.33618 ]\n",
      " [ 77.603294 319.04166 ]\n",
      " [ 79.63628  200.34048 ]\n",
      " [ 83.85602  228.35907 ]\n",
      " [ 87.091736 271.83557 ]\n",
      " [ 89.41565  225.0299  ]\n",
      " [ 90.02398  220.36293 ]\n",
      " [ 91.77024  304.50494 ]\n",
      " [ 92.675354 226.83557 ]\n",
      " [ 93.64695  248.29713 ]\n",
      " [ 94.94725  221.21806 ]\n",
      " [ 94.94725  221.21806 ]\n",
      " [ 98.65405  246.95526 ]\n",
      " [ 99.82252  298.93988 ]\n",
      " [103.32677  308.59918 ]\n",
      " [103.32677  308.59918 ]\n",
      " [104.48299  259.85422 ]\n",
      " [104.6614   220.97476 ]\n",
      " [106.86619  290.12967 ]\n",
      " [106.86619  290.12967 ]\n",
      " [106.93013  223.63179 ]\n",
      " [112.9315   248.9271  ]\n",
      " [112.99754  224.82417 ]\n",
      " [121.98334  222.79944 ]\n",
      " [136.85359  352.49725 ]\n",
      " [138.39388  264.59064 ]\n",
      " [139.61926  243.46338 ]\n",
      " [139.61926  243.46338 ]\n",
      " [141.6664   245.51562 ]\n",
      " [147.35065  268.96518 ]\n",
      " [148.89845  250.21764 ]\n",
      " [150.42139  227.90306 ]\n",
      " [150.42139  227.90306 ]\n",
      " [162.85385  240.08751 ]\n",
      " [166.01154  227.85335 ]\n",
      " [170.05528  199.727   ]\n",
      " [177.08754  193.88356 ]\n",
      " [178.97511  211.15863 ]\n",
      " [179.962    240.96683 ]\n",
      " [180.1211   167.18622 ]\n",
      " [181.3368   310.2466  ]\n",
      " [185.13559  202.56436 ]\n",
      " [186.90431  162.42131 ]\n",
      " [190.08913  320.017   ]\n",
      " [197.22154  319.7082  ]\n",
      " [211.15242  222.62167 ]\n",
      " [211.15242  222.62167 ]\n",
      " [218.72847  307.14764 ]\n",
      " [222.9919   260.22824 ]\n",
      " [225.81064  305.275   ]\n",
      " [225.99959  312.98593 ]\n",
      " [226.40605  239.40141 ]\n",
      " [229.63176  286.21707 ]\n",
      " [232.44653  229.26404 ]\n",
      " [233.96448  259.26657 ]\n",
      " [236.73027  225.91064 ]\n",
      " [237.14003  257.69382 ]\n",
      " [238.04321  249.81776 ]\n",
      " [244.40457  222.74367 ]\n",
      " [244.40457  222.74367 ]\n",
      " [246.79185  319.33707 ]\n",
      " [249.83621  308.28214 ]\n",
      " [251.0318   372.76367 ]\n",
      " [253.31288  211.12434 ]\n",
      " [258.45612  321.88177 ]\n",
      " [259.3178   303.1121  ]\n",
      " [259.43353  313.93277 ]\n",
      " [259.43353  313.93277 ]\n",
      " [260.2361   305.99872 ]\n",
      " [263.52618  210.32788 ]\n",
      " [263.52618  210.32788 ]\n",
      " [268.30127  214.62445 ]\n",
      " [268.76428  221.82115 ]\n",
      " [271.59998  216.01402 ]\n",
      " [271.59998  216.01402 ]\n",
      " [275.3429   225.88426 ]\n",
      " [278.34137  344.4762  ]\n",
      " [279.6923   304.23013 ]\n",
      " [290.4541   251.80865 ]\n",
      " [342.10867  360.86203 ]\n",
      " [382.8001   230.44897 ]\n",
      " [522.81647  200.30348 ]\n",
      " [583.6167   196.63509 ]] [[349.10114 213.89607]\n",
      " [353.4242  246.93484]\n",
      " [357.81915 306.92197]\n",
      " [367.08145 283.48007]\n",
      " [367.57785 248.76764]\n",
      " [366.94122 223.59349]\n",
      " [373.4102  297.99094]\n",
      " [381.22412 436.54987]\n",
      " [370.77963 219.76363]\n",
      " [379.63568 285.10254]\n",
      " [381.4758  316.5868 ]\n",
      " [383.4269  323.76526]\n",
      " [381.68094 240.27422]\n",
      " [391.10858 307.42865]\n",
      " [387.00494 237.59851]\n",
      " [387.00436 218.23898]\n",
      " [387.39874 211.27313]\n",
      " [296.91714 327.45697]\n",
      " [393.16742 331.47952]\n",
      " [397.1261  358.90283]\n",
      " [397.1261  358.90283]\n",
      " [390.9761  224.07237]\n",
      " [390.6662  215.88913]\n",
      " [392.95035 259.59546]\n",
      " [397.39432 319.97278]\n",
      " [402.4401  278.51486]\n",
      " [399.21866 210.7093 ]\n",
      " [400.91602 220.48273]\n",
      " [406.35797 261.9538 ]\n",
      " [408.16425 214.31213]\n",
      " [415.6915  328.6136 ]\n",
      " [413.25974 213.33966]\n",
      " [418.50656 240.08186]\n",
      " [424.80304 282.40436]\n",
      " [424.64597 236.56253]\n",
      " [425.12213 232.27972]\n",
      " [429.43387 314.7907 ]\n",
      " [428.18298 238.41368]\n",
      " [429.7691  259.79117]\n",
      " [429.97055 233.65216]\n",
      " [429.97055 233.65216]\n",
      " [434.89908 257.81296]\n",
      " [437.78043 309.1408 ]\n",
      " [440.9901  318.70486]\n",
      " [440.9901  318.70486]\n",
      " [441.16367 271.00296]\n",
      " [439.7561  232.19762]\n",
      " [444.03482 300.09573]\n",
      " [444.03482 300.09573]\n",
      " [442.5228  235.21498]\n",
      " [449.35846 259.14627]\n",
      " [448.26474 237.74742]\n",
      " [456.89258 232.36151]\n",
      " [477.67642 361.97424]\n",
      " [475.41983 273.76416]\n",
      " [474.7162  253.29558]\n",
      " [474.7162  253.29558]\n",
      " [477.71613 254.28255]\n",
      " [484.6287  277.53024]\n",
      " [485.16345 259.0536 ]\n",
      " [485.95078 237.12094]\n",
      " [485.95078 237.12094]\n",
      " [498.67938 248.59387]\n",
      " [501.8346  235.9691 ]\n",
      " [503.7847  208.4467 ]\n",
      " [510.84122 200.85637]\n",
      " [513.53424 218.40082]\n",
      " [516.72894 247.67577]\n",
      " [511.9883  173.8024 ]\n",
      " [519.87225 318.36597]\n",
      " [518.6632  209.26042]\n",
      " [519.27026 168.53331]\n",
      " [528.95703 328.18433]\n",
      " [536.4479  327.49548]\n",
      " [547.3692  227.8935 ]\n",
      " [547.3692  227.8935 ]\n",
      " [560.63513 312.30673]\n",
      " [561.4276  266.07272]\n",
      " [565.68195 311.99387]\n",
      " [566.064   320.48062]\n",
      " [562.75616 244.44963]\n",
      " [569.3229  292.4716 ]\n",
      " [569.9032  233.5673 ]\n",
      " [572.5702  265.04514]\n",
      " [573.3143  230.01765]\n",
      " [576.4556  262.90918]\n",
      " [576.87476 254.84972]\n",
      " [581.291   225.86125]\n",
      " [581.291   225.86125]\n",
      " [587.8488  326.1963 ]\n",
      " [591.20844 314.80496]\n",
      " [593.1326  382.62186]\n",
      " [615.2452  203.44603]\n",
      " [600.7357  328.89722]\n",
      " [601.20123 308.92636]\n",
      " [601.6605  320.7016 ]\n",
      " [601.6605  320.7016 ]\n",
      " [602.1178  312.4914 ]\n",
      " [601.88367 212.16011]\n",
      " [601.88367 212.16011]\n",
      " [607.0397  216.31627]\n",
      " [607.892   223.97144]\n",
      " [610.54254 217.57741]\n",
      " [610.54254 217.57741]\n",
      " [614.7729  227.59337]\n",
      " [624.4101  352.46298]\n",
      " [622.6317  309.74057]\n",
      " [633.60016 253.82825]\n",
      " [574.5443  378.69214]\n",
      " [187.71667 192.48956]\n",
      " [615.2452  203.44603]\n",
      " [627.78375 198.85625]]\n"
     ]
    }
   ],
   "source": [
    "print(src_pts.shape, dst_pts.shape)\n",
    "print(len(src_pts), len(dst_pts))\n",
    "print((src_pts), dst_pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.10766554e+00 -6.81310226e-02 -1.57422329e+02]\n",
      " [ 1.05351620e-01  1.06748416e+00 -2.58863170e+01]\n",
      " [ 4.27960510e-04 -8.94591929e-05  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Load two images\n",
    "image_1 = cv2.imread('PA120275.JPG')#warped_image1__\n",
    "image_2 = cv2.imread('PA120274.JPG')\n",
    "\n",
    "# Resize the image\n",
    "image1 = resizeImage(image_1, 50)\n",
    "# Resize the image\n",
    "image2 = resizeImage(image_2, 50)\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract keypoints for the good matches\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate homography using RANSAC\n",
    "homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "print(homography)\n",
    "\n",
    "# Apply homography to image1 to warp it onto image2\n",
    "height, width, _ = image1.shape\n",
    "warped_image2 = cv2.warpPerspective(image1, homography, (image2.shape[1] +image1.shape[1], height))\n",
    "warped_image2[0:image2.shape[0],image2.shape[1]+0:image2.shape[1]+image2.shape[1]]=image2\n",
    "\n",
    "\n",
    "def trim(frame):\n",
    "    if not np.sum(frame[0]):\n",
    "        return trim(frame[1:])\n",
    "    if not np.sum(frame[-1]):\n",
    "        return trim(frame[:-2])\n",
    "    if not np.sum(frame[:,0]):\n",
    "        return trim(frame[:,1:])\n",
    "    if not np.sum(frame[:,-1]):\n",
    "        return trim(frame[:,:-2])\n",
    "    return frame\n",
    "\n",
    "warped_image2__=trim(warped_image2)\n",
    "# Display the result\n",
    "cv2.imshow('RANSAC Matches', warped_image2__)\n",
    "# cv2.imwrite('wraped_image34.jpg',warped_image2__)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.92796102e-01  7.15257514e-02  1.42810313e+02]\n",
      " [-9.97424121e-02  9.91524262e-01  9.95413830e+00]\n",
      " [-4.06112248e-04  7.38492388e-05  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Load two images\n",
    "image_1 = warped_image2__ # cv2.imread('PA120274.JPG')\n",
    "image_2 = cv2.imread('PA120275.JPG')\n",
    "\n",
    "# Resize the image\n",
    "image1 = resizeImage(image_1, 100)\n",
    "# Resize the image\n",
    "image2 = resizeImage(image_2, 50)\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract keypoints for the good matches\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate homography using RANSAC\n",
    "homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "print(homography)\n",
    "\n",
    "# Apply homography to image1 to warp it onto image2\n",
    "height, width, _ = image1.shape\n",
    "warped_image3 = cv2.warpPerspective(image1, homography, (image2.shape[1] +image1.shape[1], height))\n",
    "warped_image3[0:image2.shape[0],0:image2.shape[1]]=image2\n",
    "\n",
    "def trim(frame):\n",
    "    if not np.sum(frame[0]):\n",
    "        return trim(frame[1:])\n",
    "    if not np.sum(frame[-1]):\n",
    "        return trim(frame[:-2])\n",
    "    if not np.sum(frame[:,0]):\n",
    "        return trim(frame[:,1:])\n",
    "    if not np.sum(frame[:,-1]):\n",
    "        return trim(frame[:,:-2])\n",
    "    return frame\n",
    "\n",
    "warped_image3__=trim(warped_image3)\n",
    "# Display the result\n",
    "cv2.imshow('RANSAC Matches', warped_image3__)\n",
    "# cv2.imwrite('wraped_image45.jpg',warped_image3__)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.60745240e-01  3.81370876e-02  1.35516973e+02]\n",
      " [-1.13197652e-01  9.25502404e-01  1.73679962e+01]\n",
      " [-3.95276107e-04 -1.46781473e-04  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Load two images\n",
    "image_1 = cv2.imread('waraped_image23.jpg')\n",
    "image_2 = cv2.imread('wraped_image34.jpg')\n",
    "\n",
    "# Resize the image\n",
    "image1 = resizeImage(image_1, 100)\n",
    "# Resize the image\n",
    "image2 = resizeImage(image_2, 100)\n",
    "\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract keypoints for the good matches\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate homography using RANSAC\n",
    "homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "print(homography)\n",
    "\n",
    "# Apply homography to image1 to warp it onto image2\n",
    "height, width, _ = image1.shape\n",
    "warped_image4 = cv2.warpPerspective(image1, homography, (image2.shape[1] +image1.shape[1], height))\n",
    "warped_image4[0:image2.shape[0],0:image2.shape[1]]=image2\n",
    "\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('RANSAC Matches', warped_image4)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aryan's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m MIN_MATCH_COUNT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(good_match) \u001b[38;5;241m>\u001b[39m MIN_MATCH_COUNT:\n\u001b[1;32m---> 31\u001b[0m   src_points \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mfloat32([kp1[m\u001b[38;5;241m.\u001b[39mqueryIdx]\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m good_match])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     32\u001b[0m   dst_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32([kp2[m\u001b[38;5;241m.\u001b[39mqueryIdx]\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m good_match])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     33\u001b[0m   H,_ \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mfindHomography(src_points, dst_points, cv2\u001b[38;5;241m.\u001b[39mRANSAC, \u001b[38;5;241m5.0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# ek baar tu isko apne file pe run kar ke dekh sajta hai kya? samajh nahi aa raha kya galat ho raha hai...\n",
    "import cv2\n",
    "\n",
    "image1 = cv2.imread('PA120272.JPG')\n",
    "image1 = cv2.resize(image1, (0,0), fx=1, fy=1)\n",
    "image_1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "image2 = cv2.imread('PA120273.JPG')\n",
    "image2 = cv2.resize(image2, (0,0), fx=1, fy=1)\n",
    "image_2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "kp1, des1 = sift.detectAndCompute(image_1, None)\n",
    "kp2, des2 = sift.detectAndCompute(image_2, None)\n",
    "\n",
    "# cv2_imshow(cv2.drawKeypoints(image1, kp1, None))\n",
    "match = cv2.BFMatcher()\n",
    "matches = match.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# draw_parameters = dict(matchColor=(0,255,0),\n",
    "#                        singlePointColor=None,\n",
    "#                        flags = 2)\n",
    "\n",
    "good_match = []\n",
    "for m,n in matches:\n",
    "  if m.distance < 0.3*n.distance:\n",
    "    good_match.append(m)\n",
    "\n",
    "MIN_MATCH_COUNT = 10\n",
    "if len(good_match) > MIN_MATCH_COUNT:\n",
    "  src_points = np.float32([kp1[m.queryIdx].pt for m in good_match]).reshape(-1,1,2)\n",
    "  dst_points = np.float32([kp2[m.queryIdx].pt for m in good_match]).reshape(-1,1,2)\n",
    "  H,_ = cv2.findHomography(src_points, dst_points, cv2.RANSAC, 5.0)\n",
    "  h,w = image_1.shape\n",
    "  pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "  # dst = cv2.perspectiveTransform(pts,H)\n",
    "  # image2 = cv2.polylines(image2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "  # cv2_imshow(image2)\n",
    "  dst = cv2.warpPerspective(image1,H,(image1.shape[1] + image2.shape[1], image2.shape[0]))\n",
    "  dst[0:image1.shape[0],0:image1.shape[1]] = image2\n",
    "  \n",
    "  def trim(frame):\n",
    "    #crop top\n",
    "    if not np.sum(frame[0]):\n",
    "        return trim(frame[1:])\n",
    "    #crop top\n",
    "    if not np.sum(frame[-1]):\n",
    "        return trim(frame[:-2])\n",
    "    #crop top\n",
    "    if not np.sum(frame[:,0]):\n",
    "        return trim(frame[:,1:])\n",
    "    #crop top\n",
    "    if not np.sum(frame[:,-1]):\n",
    "        return trim(frame[:,:-2])\n",
    "    return frame\n",
    "  cv2.imshow('frame',trim(dst))\n",
    "\n",
    "\n",
    "else:\n",
    "  print(\"No match found\")\n",
    "#cv2_imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".enpm673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
