{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENPM673 Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0 Frames per second\n"
     ]
    }
   ],
   "source": [
    "#importing modules\n",
    "from ast import Pass\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Creating a VideoCapture object and reading from input file\n",
    "cap = cv2.VideoCapture('proj2_v2.mp4')\n",
    "#capturing the fps of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "#creating frame counter\n",
    "frame_counter=0\n",
    "print(f\"{fps} Frames per second\")\n",
    "\n",
    "# Checking if camera opened successfully\n",
    "if (cap.isOpened()== False):\n",
    "  print(\"Error opening video stream or file\")\n",
    "\n",
    "# Reading until video is completed\n",
    "while(cap.isOpened()):\n",
    "  # Captureing frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    "    #incrementing frame counter\n",
    "    \n",
    "    #Converting BGR to Graysclae to reduce the channel of colors\n",
    "    image=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Variance of Laplacian\n",
    "    # laplcian=cv2.Laplacian(frame,cv2.CV_64F)\n",
    "    # laplacian_var=cv2.Laplacian(image,cv2.CV_64F).var()\n",
    "\n",
    "    #******************************\n",
    "    # Apply identity kernel\n",
    "    kernel1 = np.array([[0, 1, 0],\n",
    "                        [1, -4, 1],\n",
    "                        [0, 1, 0]])\n",
    " \n",
    "    identity = cv2.filter2D(src=image, ddepth=-1, kernel=kernel1)\n",
    "    identity_var=identity.var()\n",
    "    #updating variable text based on the threshold of variance of laplacian\n",
    "    if identity_var>150:\n",
    "        text=\"not blurry\"\n",
    "        frame_counter+=1\n",
    "        #putting text on the frame\n",
    "        # cv2.putText(frame, \"{}: {:.2f}\".format(text, laplacian_var), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 3)\n",
    "        #Smoothing the frames to eliminate the noise from the video frames\n",
    "        # gblur = cv2.GaussianBlur(image, (15, 15), 0)\n",
    "        # #--------------------------------------------------------------------\n",
    "        # #Binary thresolding a grayscale frame to capture the moving dark object\n",
    "        # ret,thres=cv2.threshold(image,205,255,cv2.THRESH_BINARY)\n",
    "        # thres_lower = 500 # Lower Threshold \n",
    "        # thres_upper = 1520 # Upper threshold \n",
    "        \n",
    "        # # Applying the Canny Edge filter with L2Gradient = True \n",
    "        # edge = cv2.Canny(thres, thres_lower, thres_upper) \n",
    "        # lines = cv2.HoughLinesP(\n",
    "        #     edge, # Input edge image\n",
    "        #     1, # Distance resolution in pixels\n",
    "        #     np.pi/180, # Angle resolution in radians\n",
    "        #     threshold=70, # Min number of votes for valid line\n",
    "        #     minLineLength=80, # Min allowed length of line\n",
    "        #     maxLineGap=10 # Max allowed gap between line for joining them\n",
    "        #     )\n",
    "        # # Iterate over points\n",
    "        # for points in lines:\n",
    "        #       # Extracted points nested in the list\n",
    "        #     x1,y1,x2,y2=points[0]\n",
    "        #     # Draw the lines joing the points\n",
    "        #     # On the original image\n",
    "        #     cv2.line(frame,(x1,y1),(x2,y2),(0,255,0),4)\n",
    "        #-------------------------------------------------------------------------------------------\n",
    "        # cv2.imshow('Frame',identity)\n",
    "    elif identity_var<150:\n",
    "      text=\"Blurry\"\n",
    "    #putting text on the frame\n",
    "    cv2.putText(identity, \"{}: {:.2f}\".format(text, identity_var), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 3)\n",
    "    cv2.imshow('Frame',identity)\n",
    "       \n",
    "    # print(laplacian_var)\n",
    "    # Press Q on keyboard to  exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "      break\n",
    "\n",
    "  # Break the loop\n",
    "  else:\n",
    "    break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Apply identity kernel\n",
    "kernel1 = np.array([[0, 0, 0],\n",
    "                    [0, 1, 0],\n",
    "                    [0, 0, 0]])\n",
    " \n",
    "identity = cv2.filter2D(src=image, ddepth=-1, kernel=kernel1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
    "# We convert the resolutions from float to integer.\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    " \n",
    "# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\n",
    "out = cv2.VideoWriter('outpy.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    " \n",
    " #in while loop\n",
    " # Write the frame into the file 'output.avi'\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0 Frames per second\n"
     ]
    }
   ],
   "source": [
    "#importing modules\n",
    "from ast import Pass\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def findIntersection(points1,points2):\n",
    "        x1,y1,x2,y2=points1[0]\n",
    "        x3,y3,x4,y4=points2[0]\n",
    "        deno_x=(x1-x2)*(y3-y4)-(y1-y2)*(x3-x4)\n",
    "        num_x=(x1*y2-y1*x2)*(x3-x4)-(x1-x2)*(x3*y4-y3*x4) \n",
    "        deno_y=(x1-x2)*(y3-y4)-(y1-y2)*(x3-x4)\n",
    "        num_y=(x1*y2-y1*x2)*(y3-y4)-(y1-y2)*(x3*y4-y3*x4)\n",
    "        if deno_x!=0:\n",
    "          px= ( num_x ) / ( deno_x ) \n",
    "        else:\n",
    "           return ['nan','nan' ]\n",
    "        if deno_y!=0:\n",
    "          py= ( num_y ) / ( deno_y )\n",
    "        else:\n",
    "           return ['nan','nan' ]\n",
    "        if np.isinf(px) or np.isinf(py):\n",
    "           return ['nan','nan' ]\n",
    "        else:\n",
    "          return [int(px), int(py)]\n",
    "\n",
    "# Creating a VideoCapture object and reading from input file\n",
    "cap = cv2.VideoCapture('proj2_v2.mp4')\n",
    "#capturing the fps of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "#creating frame counter\n",
    "frame_counter=0\n",
    "print(f\"{fps} Frames per second\")\n",
    "\n",
    "# Checking if camera opened successfully\n",
    "if (cap.isOpened()== False):\n",
    "  print(\"Error opening video stream or file\")\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    " \n",
    "# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\n",
    "out = cv2.VideoWriter('outvideo.mp4',cv2.VideoWriter_fourcc(*'mp4v'), 10, (frame_width,frame_height))\n",
    "\n",
    "# Reading until video is completed\n",
    "while(cap.isOpened()):\n",
    "  # Captureing frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    "    #incrementing frame counter\n",
    "    \n",
    "    #Converting BGR to Graysclae to reduce the channel of colors\n",
    "    image=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Variance of Laplacian\n",
    "    # laplcian=cv2.Laplacian(frame,cv2.CV_64F)\n",
    "    # laplacian_var=cv2.Laplacian(image,cv2.CV_64F).var()\n",
    "\n",
    "    #******************************\n",
    "    # Apply identity kernel\n",
    "    kernel= np.array([[1, 1, 1],\n",
    "                        [1, 1, 1],\n",
    "                        [1, 1, 1]])\n",
    "    kernel1 = np.array([[0, 1, 0],\n",
    "                        [1, -4, 1],\n",
    "                        [0, 1, 0]])\n",
    "    kernel2 = np.array([[0, -1, 0],\n",
    "                        [-1, 4, -1],\n",
    "                        [0, -1, 0]])\n",
    " \n",
    "    identity = cv2.filter2D(src=image, ddepth=cv2.CV_64F, kernel=kernel1)\n",
    "    identity_var=identity.var()\n",
    "    #updating variable text based on the threshold of variance of laplacian\n",
    "    if identity_var>150:\n",
    "        text=\"not blurry\"\n",
    "        frame_counter+=1\n",
    "        #putting text on the frame\n",
    "        # cv2.putText(frame, \"{}: {:.2f}\".format(text, laplacian_var), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 3)\n",
    "        #Smoothing the frames to eliminate the noise from the video frames\n",
    "        # gblur = cv2.GaussianBlur(image, (15, 15), 0)\n",
    "        # #--------------------------------------------------------------------\n",
    "        # #Binary thresolding a grayscale frame to capture the moving dark object\n",
    "        thres=np.where(image>=205, 255,0).astype(np.uint8)\n",
    "        # ret,thres=cv2.threshold(image,205,255,cv2.THRESH_BINARY)\n",
    "        img_ero = cv2.erode(thres, kernel, iterations=1)\n",
    "        img_dil = cv2.dilate(img_ero, kernel, iterations=1)\n",
    "\n",
    "        thres_lower = 80 # Lower Threshold \n",
    "        thres_upper = 240 # Upper threshold \n",
    "        \n",
    "        # Applying the Canny Edge filter with L2Gradient = True \n",
    "        edge = cv2.Canny(img_dil, thres_lower, thres_upper) \n",
    "\n",
    "        # This returns an array of r and theta values\n",
    "        # lines = cv2.HoughLines(edge, 1, np.pi/180, 70)\n",
    "        \n",
    "        # # print(lines)\n",
    "        # # The below for loop runs till r and theta values\n",
    "        # # are in the range of the 2d array\n",
    "        # for r_theta in lines:\n",
    "        #     arr = np.array(r_theta[0], dtype=np.float64)\n",
    "        #     r, theta = arr\n",
    "        #     # Stores the value of cos(theta) in a\n",
    "        #     a = np.cos(theta)\n",
    "        \n",
    "        #     # Stores the value of sin(theta) in b\n",
    "        #     b = np.sin(theta)\n",
    "        \n",
    "        #     # x0 stores the value rcos(theta)\n",
    "        #     x0 = a*r\n",
    "        \n",
    "        #     # y0 stores the value rsin(theta)\n",
    "        #     y0 = b*r\n",
    "        \n",
    "        #     # x1 stores the rounded off value of (rcos(theta)-1000sin(theta))\n",
    "        #     x1 = int(x0 + 1000*(-b))\n",
    "        \n",
    "        #     # y1 stores the rounded off value of (rsin(theta)+1000cos(theta))\n",
    "        #     y1 = int(y0 + 1000*(a))\n",
    "        \n",
    "        #     # x2 stores the rounded off value of (rcos(theta)+1000sin(theta))\n",
    "        #     x2 = int(x0 - 1000*(-b))\n",
    "        \n",
    "        #     # y2 stores the rounded off value of (rsin(theta)-1000cos(theta))\n",
    "        #     y2 = int(y0 - 1000*(a))\n",
    "\n",
    "        #     print(x1,y1,x2,y2)\n",
    "        #     # cv2.line draws a line in img from the point(x1,y1) to (x2,y2).\n",
    "        #     # (0,0,255) denotes the colour of the line to be\n",
    "        #     # drawn. In this case, it is red.\n",
    "        #     cv2.line(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        lines = cv2.HoughLinesP(\n",
    "            edge, # Input edge image\n",
    "            1, # Distance resolution in pixels\n",
    "            np.pi/180, # Angle resolution in radians\n",
    "            threshold=60, # Min number of votes for valid line\n",
    "            minLineLength=100, # Min allowed length of line\n",
    "            maxLineGap=5# Max allowed gap between line for joining them\n",
    "            )\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # Iterate over points\n",
    "        for points in lines:\n",
    "              # Extracted points nested in the list\n",
    "            x1,y1,x2,y2=points[0]\n",
    "            # Draw the lines joing the points\n",
    "            # On the original image\n",
    "            x=0\n",
    "            y=((y2-y1)/(x2-x1))*(-x2+x)+y2\n",
    "            x_1=x\n",
    "            y_1=int(y)\n",
    "\n",
    "            x=720\n",
    "            y=((y2-y1)/(x2-x1))*(-x2+x)+y2\n",
    "           \n",
    "            x_2=x\n",
    "            y_2=int(y)\n",
    "         \n",
    "\n",
    "            # _lines.append([x_1,y_1,x_2,y_2])\n",
    "\n",
    "            cv2.line(frame,(int (x_1),int (y_1)),(int (x_2),int (y_2)),(255,0,0),1)\n",
    "            # cv2.line(frame,(int (x1),int (y1)),(int (x2),int (y2)),(0,255,0),4)\n",
    "        # print(len(lines))\n",
    "        for i in range(len(lines)-1):\n",
    "           for j in range(len(lines)-i-1):\n",
    "              corner=findIntersection(lines[i],lines[j+i+1])\n",
    "              if corner[0]=='nan' or corner[1]=='nan':\n",
    "                  continue\n",
    "              else:\n",
    "                frame = cv2.circle(frame, tuple(corner), 5, (0,0,255), -1)\n",
    "        \n",
    "        \n",
    "        img_dil = np.float32(img_dil) \n",
    "\n",
    "        # apply the cv2.cornerHarris method \n",
    "        # to detect the corners with appropriate \n",
    "        # values as input parameters \n",
    "        dest = cv2.cornerHarris(img_dil, 5, 3, 0.07)\n",
    "        \n",
    "        frame[dest > 0.01 * dest.max()]=[0, 255, 0] \n",
    "\n",
    "        \n",
    "        # frame[dest > 0.01 * dest.max()]=[0, 255, 0] \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        #-------------------------------------------------------------------------------------------\n",
    "        # print(_lines)\n",
    "        # cv2.imshow('Frame',frame)\n",
    "    \n",
    "    #putting text on the frame\n",
    "    # cv2.putText(identity, \"{}: {:.2f}\".format(text, identity_var), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 3)\n",
    "    # cv2.imshow('Frame',identity)\n",
    "    # cv2.imshow('Frame',frame) \n",
    "\n",
    "    out.write(frame)\n",
    "    # print(laplacian_var)\n",
    "    # Press Q on keyboard to  exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "      break\n",
    "\n",
    "  # Break the loop\n",
    "  else:\n",
    "    break\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harris COrner Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0 Frames per second\n"
     ]
    }
   ],
   "source": [
    "# Python program to illustrate \n",
    "# corner detection with \n",
    "# Harris Corner Detection Method \n",
    "\n",
    "# organizing imports \n",
    "import cv2 \n",
    "import numpy as np \n",
    "\n",
    "# Creating a VideoCapture object and reading from input file\n",
    "cap = cv2.VideoCapture('proj2_v2.mp4')\n",
    "#capturing the fps of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "#creating frame counter\n",
    "frame_counter=0\n",
    "print(f\"{fps} Frames per second\")\n",
    "\n",
    "# Checking if camera opened successfully\n",
    "if (cap.isOpened()== False):\n",
    "  print(\"Error opening video stream or file\")\n",
    "\n",
    "# Reading until video is completed\n",
    "while(cap.isOpened()):\n",
    "  # Captureing frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    "    #incrementing frame counter\n",
    "    \n",
    "    #Converting BGR to Graysclae to reduce the channel of colors\n",
    "    image=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Variance of Laplacian\n",
    "    # laplcian=cv2.Laplacian(frame,cv2.CV_64F)\n",
    "    # laplacian_var=cv2.Laplacian(image,cv2.CV_64F).var()\n",
    "\n",
    "    #******************************\n",
    "    # Apply identity kernel\n",
    "    kernel1 = np.array([[0, 1, 0],\n",
    "                        [1, -4, 1],\n",
    "                        [0, 1, 0]])\n",
    " \n",
    "    identity = cv2.filter2D(src=image, ddepth=-1, kernel=kernel1)\n",
    "    identity_var=identity.var()\n",
    "    #updating variable text based on the threshold of variance of laplacian\n",
    "    if identity_var>90:\n",
    "        text=\"not blurry\"\n",
    "        frame_counter+=1\n",
    "         # #Binary thresolding a grayscale frame to capture the moving dark object\n",
    "        ret,thres=cv2.threshold(image,205,255,cv2.THRESH_BINARY)\n",
    "        img_ero = cv2.erode(thres, kernel, iterations=1)\n",
    "        img_dil = cv2.dilate(img_ero, kernel, iterations=1)\n",
    "        # modify the data type \n",
    "        # setting to 32-bit floating point \n",
    "        img_dil = np.float32(img_dil) \n",
    "\n",
    "        # apply the cv2.cornerHarris method \n",
    "        # to detect the corners with appropriate \n",
    "        # values as input parameters \n",
    "        dest = cv2.cornerHarris(img_dil, 2, 5, 0.05) \n",
    "\n",
    "        # Results are marked through the dilated corners \n",
    "        # dest = cv2.dilate(dest, None) \n",
    "\n",
    "        # Reverting back to the original image, \n",
    "        # with optimal threshold value \n",
    "        frame[dest > 0.01 * dest.max()]=[0, 0, 255] \n",
    "\n",
    "        # the window showing output image with corners \n",
    "        cv2.imshow('Image with Borders', frame) \n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "       break\n",
    "\n",
    "  # Break the loop\n",
    "  else:\n",
    "    break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canny edge with the trackbar\n",
    "\n",
    "##### Upper threshold: All the pixel values above threshold limit is considered\n",
    "##### Lower Threshold: All the Pixel value below Threshold limit is discarded\n",
    "##### Values in between Upp-Lower Threshold : considered only if the connected pixel is from upper thresold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "def something(x):\n",
    "    pass\n",
    "# Create a black image, a window\n",
    "img = cv.imread('book.jpg',0)\n",
    "img=cv.resize(img,(600,400),cv.INTER_AREA)\n",
    "cv.namedWindow('image')\n",
    "# create trackbars for color change\n",
    "cv.createTrackbar('min','image',0,1000,something)\n",
    "cv.createTrackbar('max','image',0,1000,something)\n",
    "# create switch for ON/OFF functionality\n",
    "switch = \"0 : OFF \\n 1 : ON\"\n",
    "cv.createTrackbar(switch, 'image',0,1,something)\n",
    "while(1):\n",
    "    # get current positions of four trackbars\n",
    "    g = cv.getTrackbarPos ('min', 'image')\n",
    "    b = cv.getTrackbarPos ('max', 'image')\n",
    "    s = cv.getTrackbarPos (switch, 'image')\n",
    "    if s == 0:\n",
    "        img=img\n",
    "        edges=img\n",
    "    if s==1:\n",
    "        edges = cv.Canny(img,g,b)\n",
    "    cv.imshow ('image', edges)\n",
    "    k = cv.waitKey (1) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "2 3\n",
      "2 4\n",
      "2 5\n",
      "3 4\n",
      "3 5\n",
      "4 5\n"
     ]
    }
   ],
   "source": [
    "pin=[1,2,3,4,5]\n",
    "for i in range(len(pin)-1):\n",
    "    for j in range(len(pin)-i-1):\n",
    "        print(pin[i],pin[j+1+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "### Homography and stitching an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualising SIFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load an image\n",
    "image = cv2.imread('PA120274.JPG')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "\n",
    "# Draw keypoints on the image\n",
    "image_with_keypoints = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('SIFT Keypoints', image_with_keypoints)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matching SIFT features between 2 consecutive images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two images\n",
    "image1 = cv2.imread('PA120275.JPG')\n",
    "image2 = cv2.imread('PA120274.JPG')\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Draw matches\n",
    "matched_image = cv2.drawMatches(image1, keypoints1, image2, keypoints2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Matched Features', matched_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing Homography between the pair of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining Resize of image\n",
    "def resizeImage(image, percentage):\n",
    "  \"\"\"Function to resize an image by given percentage\n",
    "  \"\"\"\n",
    "  original_height, original_width = image.shape[:2]\n",
    "\n",
    "  new_height = int(original_height * (percentage / 100))\n",
    "  new_width = int(original_width * (percentage / 100))\n",
    "\n",
    "  resized_img = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "  return resized_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Joining 273 and 272\n",
    "##### where 272 is source image on which homo trnasform wll be applied and 273 will be kept as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.41215807e-01  5.59591722e-02  1.64579418e+02]\n",
      " [-1.15982217e-01  9.73796679e-01  1.21197103e+01]\n",
      " [-5.50690447e-04  6.61437139e-05  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two images\n",
    "image_1 = cv2.imread('PA120272.JPG')\n",
    "image_2 = cv2.imread('PA120273.JPG')\n",
    "\n",
    "# Resize the image\n",
    "image1 = resizeImage(image_1, 50)\n",
    "# Resize the image\n",
    "image2 = resizeImage(image_2, 50)\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract keypoints for the good matches\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 2)\n",
    "\n",
    "# Estimate homography using RANSAC\n",
    "homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "print(homography)\n",
    "\n",
    "# Apply homography to image1 to warp it onto image2\n",
    "height, width, _ = image1.shape\n",
    "warped_image1 = cv2.warpPerspective(image1, homography, (image2.shape[1] +image1.shape[1], height))\n",
    "warped_image1[:image2.shape[0],:image2.shape[1]]=image2\n",
    "\n",
    "\n",
    "def trim(frame):\n",
    "    if not np.sum(frame[0]):\n",
    "        return trim(frame[1:])\n",
    "    if not np.sum(frame[-1]):\n",
    "        return trim(frame[:-2])\n",
    "    if not np.sum(frame[:,0]):\n",
    "        return trim(frame[:,1:])\n",
    "    if not np.sum(frame[:,-1]):\n",
    "        return trim(frame[:,:-2])\n",
    "    return frame\n",
    "\n",
    "\n",
    "warped_image1__=trim(warped_image1)\n",
    "# Display the result\n",
    "cv2.imshow('RANSAC Matches', warped_image1__)\n",
    "cv2.imwrite('warped_image23.jpg', warped_image1__)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Joining 275 and 274\n",
    "##### where 275 is source image on which homo trnasform wll be applied and 274 will be kept as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.10766554e+00 -6.81310226e-02 -1.57422329e+02]\n",
      " [ 1.05351620e-01  1.06748416e+00 -2.58863170e+01]\n",
      " [ 4.27960510e-04 -8.94591929e-05  1.00000000e+00]]\n",
      "[[ 1.10766554e+00 -6.81310226e-02  1.80000000e+02]\n",
      " [ 1.05351620e-01  1.06748416e+00 -2.58863170e+01]\n",
      " [ 4.27960510e-04 -8.94591929e-05  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Load two images\n",
    "image_1 = cv2.imread('PA120275.JPG')#warped_image1__\n",
    "image_2 = cv2.imread('PA120274.JPG')\n",
    "\n",
    "# Resize the image\n",
    "image1 = resizeImage(image_1, 50)\n",
    "# Resize the image\n",
    "image2 = resizeImage(image_2, 50)\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract keypoints for the good matches\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate homography using RANSAC\n",
    "homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "print(homography)\n",
    "homo=homography\n",
    "#need to trnaslate the transformed image to mix with destination image\n",
    "homo[0,2]=180\n",
    "print(homo)\n",
    "\n",
    "# Apply homography to image1 to warp it onto image2\n",
    "height, width, _ = image1.shape\n",
    "warped_image2 = cv2.warpPerspective(image1, homo, (image2.shape[1] +image1.shape[1], height))\n",
    "\n",
    "#here we are adding image from half way through hence mentioned:\n",
    "#if image is 640x240\n",
    "# image[0:240, 320:640 ]= image_other\n",
    "warped_image2[0:image2.shape[0],image2.shape[1]+0:image2.shape[1]+image2.shape[1]]=image2\n",
    "\n",
    "\n",
    "\n",
    "def trim(frame):\n",
    "    if not np.sum(frame[0]):\n",
    "        return trim(frame[1:])\n",
    "    if not np.sum(frame[-1]):\n",
    "        return trim(frame[:-2])\n",
    "    if not np.sum(frame[:,0]):\n",
    "        return trim(frame[:,1:])\n",
    "    if not np.sum(frame[:,-1]):\n",
    "        return trim(frame[:,:-2])\n",
    "    return frame\n",
    "\n",
    "warped_image2__=trim(warped_image2)\n",
    "# Display the result\n",
    "cv2.imshow('RANSAC Matches', warped_image2__)\n",
    "# cv2.imwrite('wraped_image34.jpg',warped_image2__)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### wrapping the above two images into one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.03222859e-01  7.55191131e-03  2.88727329e+02]\n",
      " [-1.11867591e-01  9.19336610e-01  1.78586813e+01]\n",
      " [-3.83902182e-04 -1.66034364e-04  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Load two images\n",
    "image_1 = warped_image1__ # cv2.imread('PA120274.JPG')\n",
    "image_2 = warped_image2__\n",
    "\n",
    "# Resize the image\n",
    "image1 = resizeImage(image_1, 100)\n",
    "# Resize the image\n",
    "image2 = resizeImage(image_2, 100)\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract keypoints for the good matches\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate homography using RANSAC\n",
    "homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "print(homography)\n",
    "# homo=homography\n",
    "# homo[0,2]=200\n",
    "\n",
    "# Apply homography to image1 to warp it onto image2\n",
    "height, width, _ = image1.shape\n",
    "warped_image3 = cv2.warpPerspective(image1, homography, (image2.shape[1] +image1.shape[1], height))\n",
    "warped_image3[0:image2.shape[0],0:image2.shape[1]]=image2\n",
    "# warped_image3[0:image2.shape[0],image1.shape[1]+0:image2.shape[1]+image1.shape[1]]=image2\n",
    "\n",
    "def trim(frame):\n",
    "    if not np.sum(frame[0]):\n",
    "        return trim(frame[1:])\n",
    "    if not np.sum(frame[-1]):\n",
    "        return trim(frame[:-2])\n",
    "    if not np.sum(frame[:,0]):\n",
    "        return trim(frame[:,1:])\n",
    "    if not np.sum(frame[:,-1]):\n",
    "        return trim(frame[:,:-2])\n",
    "    return frame\n",
    "\n",
    "warped_image3__=trim(warped_image3)\n",
    "# Display the result\n",
    "cv2.imshow('RANSAC Matches', warped_image3__)\n",
    "cv2.imwrite('wraped_full.jpg',warped_image3__)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".enpm673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
